{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a1eb0a-287f-4058-bdd5-05db3d95bea9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  4 09:38:40 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 595.02                 Driver Version: 595.02         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 58%   32C    P0             37W /  200W |    1509MiB /  12282MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1620    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A            2360    C+G   ....0.3650.96\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A            3432    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A            7064    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A           11136    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           11596    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A           11608    C+G   ....0.3650.96\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           13060    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           24632    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A           25260    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           25300    C+G   ...8wekyb3d8bbwe\\M365Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A           26696    C+G   ....0.3650.96\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           28452    C+G   ...26wp6bftszj\\TranslucentTB.exe      N/A      |\n",
      "|    0   N/A  N/A           29752    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           30872    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           31068    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           31612    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           32772    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           33492    C+G   ...2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A           33848    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           37196    C+G   ...lpaper Engine\\wallpaper64.exe      N/A      |\n",
      "|    0   N/A  N/A           38168    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           39128    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A           39492    C+G   ...yb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12476dc5-f1d2-4b29-a6e3-df2377585051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974b23efda26432e92e37b023d706a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74ef9f86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '6884f9eeb4d898c5a7a7713f',\n",
       " 'name': 'eleviacorp',\n",
       " 'fullname': 'Rehan',\n",
       " 'email': 'eleviacorporation@gmail.com',\n",
       " 'emailVerified': True,\n",
       " 'canPay': False,\n",
       " 'billingMode': 'prepaid',\n",
       " 'periodEnd': 1767225600,\n",
       " 'isPro': False,\n",
       " 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8SQ2Tu3BxC5t82VEmvBUK.png',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'elenevread',\n",
       "   'role': 'read',\n",
       "   'createdAt': '2025-12-22T02:51:37.899Z'}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0424217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import convokit\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,   \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e36e92-9614-4f5c-8be4-e637c7895ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2355c888-2ded-4615-8ad2-4acd258445e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d520251e814d16be975fe3f1391c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LORA_PATH = \"./lora-llama3-rizz/roast/checkpoint-815\" \n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb,\n",
    "    device_map={\"\": 0}\n",
    ")   \n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99559619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fba32cd258a42338fdcd0d04250f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LORA_PATH = \"./lora-llama3-rizz/Training2/checkpoint-523\" \n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\"\n",
    ")   \n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_PATH,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103a4d6f-6634-4952-a612-fae2612dd6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Total params: 8043892736\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device:\", next(base_model.parameters()).device)\n",
    "print(\"Total params:\", base_model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79408d6c-db4e-428b-8465-7b88b9c0eb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 88451 messages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "CHAT_FOLDER = \"instagram_chats\"\n",
    "\n",
    "all_messages = []\n",
    "\n",
    "for filename in os.listdir(CHAT_FOLDER):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(CHAT_FOLDER, filename)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        chat = json.load(f)\n",
    "\n",
    "    participants = [p[\"name\"] for p in chat.get(\"participants\", [])]\n",
    "\n",
    "    for msg in chat.get(\"messages\", []):\n",
    "        text = msg.get(\"content\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        all_messages.append({\n",
    "            \"chat_file\": filename,\n",
    "            \"participants\": participants,\n",
    "            \"sender\": msg[\"sender_name\"],\n",
    "            \"text\": text,\n",
    "            \"timestamp\": msg[\"timestamp_ms\"]\n",
    "        })\n",
    "\n",
    "print(f\"loaded {len(all_messages)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f38912d5-b90b-4bf2-9f47-14516117207a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 40961 messages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "CHAT_FOLDER = \"new_insta_chats\"\n",
    "\n",
    "all_messages = []\n",
    "\n",
    "for filename in os.listdir(CHAT_FOLDER):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(CHAT_FOLDER, filename)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        chat = json.load(f)\n",
    "\n",
    "    participants = [p[\"name\"] for p in chat.get(\"participants\", [])]\n",
    "\n",
    "    for msg in chat.get(\"messages\", []):\n",
    "        text = msg.get(\"content\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        all_messages.append({\n",
    "            \"chat_file\": filename,\n",
    "            \"participants\": participants,\n",
    "            \"sender\": msg[\"sender_name\"],\n",
    "            \"text\": text,\n",
    "            \"timestamp\": msg[\"timestamp_ms\"]\n",
    "        })\n",
    "\n",
    "print(f\"loaded {len(all_messages)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1f2e5c-4c95-4ae4-8e6a-6fb7cfbf8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_emoji(text):\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1381b0-e85f-4e6c-a4a6-bb9325beb0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Messages: 69410\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_real_text(text):\n",
    "    t = text.lower().strip()\n",
    "\n",
    "    junk_phrases = [\n",
    "        \"sent an attachment\",\n",
    "        \"reacted\",\n",
    "        \"you sent an attachment\",\n",
    "        \"liked a message\",\n",
    "        \"unsent a message\",\n",
    "    ]\n",
    "\n",
    "    if any(j in t for j in junk_phrases):\n",
    "        return False\n",
    "    if len(t) < 3:\n",
    "        return False\n",
    "    if re.fullmatch(r\"[^\\w\\s]+\", t):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "clean_messages = []\n",
    "\n",
    "for m in all_messages:\n",
    "    fixed_text = fix_emoji(m[\"text\"])\n",
    "\n",
    "    if is_real_text(fixed_text):\n",
    "        m[\"text\"] = fixed_text\n",
    "        clean_messages.append(m)\n",
    "\n",
    "print(f\"Clean Messages: {len(clean_messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859c4c53-1004-4558-b613-02dc99f19a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_USERNAME = \"Rehan Raza\"\n",
    "\n",
    "for m in clean_messages:\n",
    "    m[\"is_me\"] = (m[\"sender\"] == MY_USERNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bb710e-3a59-4028-a4cc-01e443748718",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_messages_sorted = sorted(clean_messages, key=lambda x: x[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726483a6-47d7-482f-8f7a-058c1c390bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 4270 conversations\n"
     ]
    }
   ],
   "source": [
    "def build_conversations(messages):\n",
    "    conversation = []\n",
    "    current = []\n",
    "    last_chat = None\n",
    "\n",
    "    for m in messages:\n",
    "        if last_chat and m[\"chat_file\"] != last_chat:\n",
    "            if len(current) >= 2:\n",
    "                conversation.append({\"messages\": current})\n",
    "            current = []\n",
    "\n",
    "        role = \"assistant\" if m[\"is_me\"] else \"user\"\n",
    "        current.append({\"role\": role, \"content\": m[\"text\"]})\n",
    "        last_chat = m[\"chat_file\"]\n",
    "\n",
    "    if len(current) >= 2:\n",
    "        conversation.append({\"messages\": current})\n",
    "    return conversation\n",
    "\n",
    "conversations = build_conversations(clean_messages_sorted)\n",
    "print(f\"Built {len(conversations)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad339adb-3baa-4c18-89b0-531839297dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved All Conversations\n"
     ]
    }
   ],
   "source": [
    "with open(\"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved All Conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbcf484-4723-422b-a6ef-ba7514370d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_assistants(conversations):\n",
    "    fixed_convs = []\n",
    "\n",
    "    for conv in conversations:\n",
    "        merged = []\n",
    "        buffer = None\n",
    "\n",
    "        for msg in conv[\"messages\"]:\n",
    "            if msg[\"role\"] == \"assistant\":\n",
    "                if buffer is None:\n",
    "                    buffer = msg[\"content\"]\n",
    "                else:\n",
    "                    buffer += \" \" + msg[\"content\"]\n",
    "\n",
    "            else:\n",
    "                if buffer is not None:\n",
    "                    merged.append({\"role\": \"assistant\", \"content\": buffer})\n",
    "                    buffer = None\n",
    "                merged.append(msg)\n",
    "\n",
    "        if buffer is not None:\n",
    "            merged.append({\"role\": \"assistant\", \"content\": buffer})\n",
    "\n",
    "        fixed_convs.append({\"messages\": merged})\n",
    "\n",
    "    return fixed_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8666e5d6-14b3-4f24-bc09-1554f644db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant messages merged\n",
      "Saved All Conversations\n"
     ]
    }
   ],
   "source": [
    "conversations = merge_consecutive_assistants(conversations)\n",
    "print(\"Assistant messages merged\")\n",
    "\n",
    "with open(\"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved All Conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c66188-9ef6-4d28-bea8-32e359c374fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0a4f20-5e9e-4985-bd7f-d9929aea4d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "dict_keys(['messages'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    new_convs = json.load(f)\n",
    "\n",
    "type(new_convs)\n",
    "\n",
    "print(type(new_convs))\n",
    "print(type(new_convs[0]))\n",
    "print(new_convs[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0ba841-f072-460c-ad27-b30ab5785844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_leading_assistant(conversations):\n",
    "    fixed = []\n",
    "    for conv in conversations:\n",
    "        msgs = conv[\"messages\"]\n",
    "        while msgs and msgs[0][\"role\"] == \"assistant\":\n",
    "            msgs = msgs[1:]\n",
    "        if len(msgs) >= 2:\n",
    "            fixed.append({\"messages\": msgs})\n",
    "    return fixed\n",
    "\n",
    "conversations = drop_leading_assistant(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "780f719d-dbca-49c9-a9d0-7db424c631ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant messages merged\n",
      "Saved All Conversations\n"
     ]
    }
   ],
   "source": [
    "conversations = merge_consecutive_assistants(conversations)\n",
    "print(\"Assistant messages merged\")\n",
    "\n",
    "with open(\"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conversations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved All Conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec3be09-7abe-4cce-988d-87d0ea3b67fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min tokens: 39\n",
      "Max tokens: 1024\n",
      "Avg tokens: 176\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for conv in conversations:\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        conv[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    token_ids = tokenizer(\n",
    "        chat_text,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    lengths.append(len(token_ids))\n",
    "\n",
    "\n",
    "print(\"Min tokens:\", min(lengths))\n",
    "print(\"Max tokens:\", max(lengths))\n",
    "print(\"Avg tokens:\", sum(lengths) // len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8512b4c6-64cc-4e97-bb23-831cd6087fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    out = tokenizer(\n",
    "        chat_text,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d3e4c1-8374-4e54-851b-ac20219cbc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at C:\\Users\\pc\\.convokit\\saved-corpora\\subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "import random\n",
    "\n",
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))\n",
    "\n",
    "rd_convos = list(corpus.iter_conversations())\n",
    "random.shuffle(rd_convos)\n",
    "\n",
    "rd_convos = rd_convos[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbda765e-846f-4800-aeb0-7630c9a4d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convo_to_messages(convo):\n",
    "    messages = []\n",
    "    for utt in convo.iter_utterances():\n",
    "        role = \"user\" if len(messages) % 2 == 0 else \"assistant\"\n",
    "        messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": utt.text.strip()\n",
    "        })\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a4c69b-e5fb-47fb-bb45-0c02f684d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good(messages):\n",
    "    if len(messages) < 6:\n",
    "        return False\n",
    "    for m in messages:\n",
    "        text = m[\"content\"]\n",
    "        if len(m[\"content\"]) < 5:\n",
    "            return False\n",
    "        if text.lower() in [\"lol\", \"ok\", \"yes\", \"no\"]:\n",
    "            return False\n",
    "\n",
    "    for i in range(1, len(messages)):\n",
    "        if messages[i][\"role\"] == messages[i-1][\"role\"]:\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d49fc29-fec7-4bd1-abc7-13ccae04d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured = []\n",
    "\n",
    "for rd_convo in rd_convos:\n",
    "    msgs = convo_to_messages(rd_convo)\n",
    "    if is_good(msgs):\n",
    "        structured.append({\"messages\": msgs})\n",
    "\n",
    "structured = structured[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794ba50b-eb2a-44af-8b3e-6f39f2f8ce85",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : So my first semester of freshman year didn't go great, but I told myself some bullshit along the lines of I'm still adjusting to college blah blah, it's ok, I'll only go up from here, etc. Second semester was a bit better, although looking at my transcript, it's only because my classes had slightly higher median grades, I was still the same amount below the median as before... Anyway, I still felt decently optimistic and told myself third semester was gonna be the best one yet!!! Now that it's over, I've come to the conclusion that I'm simply stupid. \n",
      "It seems as though the difference between me genuinely trying to do better vs losing hope and aiming for bare minimum is pretty negligible, which isn't doing wonders for my motivation going forward...\n",
      "Anyone have any tips for trying to suck a bit less/getting out of this mindset? Also any tips for making myself seem at least semi competent when applying for internships would be appreciated.\n",
      "ASSISTANT : I actually feel exactly the same way and have had the same exact experience as you LOL.\n",
      "USER : Relatable tbh\n",
      "ASSISTANT : Are you below the median in ALL of your classes? Are you at least doing better in major-specific classes? If so, that can go a long way towards landing an internship/job. I’ve made some pretty shit grades in classes unrelated to my major but was somewhat competent where it counted, and I got my dream post-grad job. Anecdotes aren’t data, obviously, but if your major GPA is higher than your cumulative GPA, I’d play that up in your applications.\n",
      "\n",
      "Depending on your field, too, research (either independent or with a mentor) can be a big plus. It’s my understanding that aside from finance and premed-geared internships, your GPA isn’t the most important factor, as long as it’s above a 3.0 or so.\n",
      "USER : As for the getting out of the mindset part:\n",
      "&gt; It seems as though the difference between me genuinely trying to do better vs losing hope and aiming for bare minimum is pretty negligible\n",
      "\n",
      "Let's say you tried really hard to ace a class but ended up with a C. If you'd put in less effort: Best case, you learn less of the material and get the same grade. Worse (and more likely) result: you learn less of the material and get a lower grade.\n",
      "\n",
      "As for tips, if you're comfortable sharing what your study habits are like (e.g. do you form study groups, go to office hours, etc.) then hopefully we can offer some more advice.\n",
      "ASSISTANT : I'm CS and pretty much only programming classes I've taken so far have been 1110 and 2110 and I got exactly the median in both. So I'm not doing horribly in those I guess but also not sure playing up a 3.0 in two fairly intro classes is applicable.\n",
      "USER : I agree on the sake of learning part. I generally found myself thinking that about classes I didn't particularly care about (for me, physics). After putting in a genuine effort at the start of the semester, basically my logic was that trying for a slightly higher grade would be much more effort than it's worth.\n",
      "As for study habits, I'm going to try to take more advantage of office hours next semester. I've gone occasionally, but I generally try to figure things out on my own, which despite taking me probably significantly longer than it should, I feel like usually helps more in the long run since it's much easier to \"understand\" something when someone explains it you but be unable to do it yourself still (admittedly I also probably still have a mild fear of asking for help, which I need to get over asap...) I've worked on problem sets with others this semester more frequently than in previous semesters, although I can't say I enjoyed it. For some classes it didn't do much since I felt like I could've done the assignments just as efficiently on my own, although it was useful to be able to compare answers in the end. Occasionally though, whoever I was working with clearly had a much better idea of what was going on than me and I don't think I got much out of doing the assignments as I just went along with whatever they said. This is definitely partially my own fault, but I especially don't like feeling as though I'm slowing someone else down, so I'm even less likely to ask a classmate to re-explain a problem than a TA.\n",
      "My self-sufficient strategy works well for some period of time until I reach a point in the semester where I just don't have enough time to do everything I need to on my own and begin slacking on a couple assignments and never recover from that... Alright so yeah, three semesters in I'm realizing this is clearly not working for me... I'll try something different lol\n",
      "ASSISTANT : Yes, I can definitely empathize with the \"self-sufficient\" thing. \n",
      "\n",
      "Great that you've experimented with working with others, and I'm wondering if you've tried this specifically - do as much of the assignment as possible and understand as much you can by yourself, then work with other students for only the remaining bits and/or to check answers? Some of my classmates and I find this helpful not only for getting good grades on the assignments, but also for seeing how others approach the homework problems.\n",
      "USER : &gt; I got exactly the median in both.\n",
      "\n",
      "What's wrong with this exactly?\n",
      "ASSISTANT : Nothing with that part, those are some of very few classes I've done at the median in. I guess those are the more important ones, so I'm satisfied with that.\n",
      "USER : I'll try that, seems like a better approach. I usually don't offer to work with others so only times I do is when someone else suggests it which usually results in us doing the whole thing together.\n",
      "ASSISTANT : Once you get a job, you will probably look back at this comment and be quite amused. In the \"real world\" (sorry for the cliche), soft skills are just as important as grades and ability (within reason of course. If you genuinely just can't comprehend a linked list, you may want to change major). The problem is that colleges have no way of quantifying your soft skills, so they quantify you by the only things they can i.e. homeworks, exams, projects etc...... This means that a HUGE component of a person's capabilities, or lack thereof, is completely hidden. Do yourself, and your sanity a favor; stop being grade-driven, and become education-driven e.g. Instead of \"OMG, I need to get at least a B+ on this prelim to get a C in the class\", try to take a real interest in what you're learning i.e \"I need to understand how this works because it is required for my career\". Hang in there, and don't worry about comparing yourself to others; just keep getting better!\n",
      "USER : Thanks. I'm kind of at a point where I'm doubting that I've actually learned anything... Although I'm trying to work on that by going over some topics I've already covered in the classes I've taken.\n",
      "ASSISTANT : You'd be surprised at what you've learned. Just as important, you're learning how to learn. I found it helpful to do side projects where I actually had to apply the things I had learned to an actual problem; that's when things start to click. Just make a commitment to yourself to keep getting better!\n"
     ]
    }
   ],
   "source": [
    "sample = structured[0][\"messages\"]\n",
    "\n",
    "for msg in sample:\n",
    "    print(msg[\"role\"].upper(), \":\", msg[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5302a0f4-d4cf-494e-ba8d-737a32857c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(dataset, n=5):\n",
    "    for i in range(n):\n",
    "        conv = dataset[i][\"messages\"] \n",
    "        print(\"\\n--- Conversation\", i, \"---\")\n",
    "        for m in conv:\n",
    "            print(f'{m[\"role\"]}: {m[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99cc01a5-0bb5-45a6-afa4-ded7d4c7b096",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conversation 0 ---\n",
      "user: I'm completely redoing my schedule in Add/Drop, so I'm going to need to drop all my old ones in order to do the new ones.\n",
      "\n",
      "Is there a way I can drop my old classes and add my new ones in one step, or do I have to delete then add?\n",
      "assistant: You can “swap” them which will ensure that if you are not able to replace a class you already have with a desired one, the undesired one will remain. You can swap by code, class name, or directly from your shopping cart. The only downside is that you can’t swap more than one class at a time, so you can’t get them all done with a single click right as your appointment time opens.\n",
      "user: \"Swap\".\n",
      "assistant: The swap option is not there right now, unless I dont know where to look. Is it because add starts tomorrow 9 am?\n",
      "user: It should be in the same place as where you can select add and drop. It was there for me before add/deop started\n",
      "assistant: Found it but says I dont have enrollment appointment\n",
      "user: Yep, that’s normal. Just wait til add/drop opens\n",
      "assistant: And one of the DIS sections was showing open this morning and now closed. The Lec I want to swap to is still open. Any suggestions on best way to do this?\n",
      "\n",
      "--- Conversation 1 ---\n",
      "user: I'm teaching a new course this semester, CS 1380 (cross listed ORIE 1380 and STSCI 1380).  It's an introduction to Data Science that is designed to be accessible to people from any major, regardless of background.  Interested?  Come join us!  No experience required, and fulfills MQR-AS.\n",
      "\n",
      "More details here: \n",
      " [http://www.cs.cornell.edu/courses/cs1380/2018sp/](http://www.cs.cornell.edu/courses/cs1380/2018sp/)\n",
      "assistant: Hi! This course sounds interesting but I've never taken a pure CS course before. I took a stats course where I had to work with R, and I wasn't too good at it. Does this course start with the very basics or does it assume that you have a basic understanding of CS? I'm a little bit scared of being stuck in course with a bunch of CS majors and being totally lost\n",
      "user: From link\n",
      "&gt;Q: Do I need to know how to program for 1380?\n",
      "\n",
      "&gt;A: Nope! We’ll teach you everything you need to know.\n",
      "assistant: I strongly doubt that it will be a class with mostly CS majors, or even a majority of CS majors. It's not a class that most CS majors will be interested in (to the extent that they can fit it in their schedules, at least), really.\n",
      "user: Seconded ^\n",
      "This class is targeted for non-CS majors\n",
      "assistant: Which stats course did you take?  Just wondering\n",
      "user: STSCI 2150\n",
      "assistant: Do you think it is a mistake to take the class as a prospective CS major? I have some interest in Data Science so I thought it would be a cool class to take for me. However, I have taken AP Stats in high school and have completed 2110\n",
      "user: From what I know of the class (admittedly a very small amount), I doubt it would be worthwhile. If you're interested in data science you could probably just go straight into the higher level CS courses that are concerned with data science, since you've already done 2110.\n",
      "assistant: Do you know what classes are good to take? I don't know if I actually will like data science but I would like to be more knowledgeable on it\n",
      "user: SpookBusters is right:  if you've had AP stats and 2110, then 1380 is going to be too much review for you.  Here are some possibilities I know about for more advanced classes:  INFO 2950, INFO 3300, CS 4780, ORIE 4740, STSCI 4060.  There are probably more!\n",
      "assistant: We start with the very basics!\n",
      "user: Would you recommend this class for someone trying to see if the OR major is right for them? I can't fit ENGRI 1101 or ENGRD 2700 into my schedule, but really want to try out OR. I'm interested in optimization.\n",
      "assistant: This course sounds very interesting! I'm an info-sci major and I'm required to take an intrductory statistics course (a common one being STSCI 2100). Will this course fulfill that requirement?\n",
      "\n",
      "--- Conversation 2 ---\n",
      "user: I’m trying to place out of multivariable calculus, and I was wondering if anyone had any advice going into that test. What grade do I need to pass? Is it really hard? Are there any online resources I can use to prepare?\n",
      "assistant: I took the test last year and passed. I don't remember what the passing grade was, I don't even think they revealed it. I only made slight mistakes on a couple questions so I probably can't give you a good idea of how tough passing was.\n",
      "\n",
      "I didn't find it that hard, I took a multi course in high school and it was on par with the multi course's exams, just slightly harder on a few parts and requiring memorizing formulas.\n",
      "\n",
      "As far as online resources go, you can google old MATH 1920 prelims/finals to get ideas of what MATH 1920 exams look like, and there's a pretty good old blackboard course for MATH 1920 (I don't remember exactly how you enroll in it, but type MATH 1920 and the result that was taught by Connolly in 2010 is the good one). It has a bunch of old prelims, homework solutions and past exams. \n",
      "\n",
      "I'm dumping all the PDFs rn, so if you're reading this in the future and blackboard is gone, PM me.\n",
      "user: Did you study for the CASE? I took a Multi course last fall and I’ve been trying to do some review because I’m worried I won’t remember it all. Should I focus specifically on memorizing formulas? Or anything else?\n",
      "assistant: that's exactly the position I was in. I definitely invested a little too much effort into formula memorization but you should know them. I recommend doing a decent amount of practice using exams and other practice problems you can find, but studying isn't as important because your course probably taught you all of multi (the only thing I really had to study was surface integrals because I forgot them).\n",
      "user: [deleted]\n",
      "assistant: Thank you so much for responding! Would you say the test is more heavily weighted towards the end of the class, like more stokes theorem than directional derivatives?\n",
      "user: I think I went a bit overboard because I started rereading and taking notes on my textbook from the beginning of the course... I’ll probably switch to just some practice problems and a bit of review of Stokes and whatnot. Thanks for the insight!\n",
      "assistant: Last question from a clueless prefrosh - how/where do I find old Blackboard courses? I'm looking around and can't find any sign of them. Thanks.\n",
      "user: I took this test two years ago and passed. I have a similar background to u/sasha07974 and found the test not too hard. The bar for passing is fairly low, I think they're looking to see on a case-by-case basis if you know your stuff at a reasonable level. If you study something like 1920 resources or Paul's notes for a refresher you'll be more than fine - I was and I took multi in junior year. The test was very practical/computational with only one question where you had to show/prove stuff and it was very easy. Don't stress about it too much.\n",
      "assistant: You don't as far as I know\n",
      "user: No, study all of it in detail.\n",
      "assistant: [removed]\n",
      "user: Thank you so much for responding!! Did you feel pressed for time at all?\n",
      "assistant: Not really, I remember being able to check my work several times\n"
     ]
    }
   ],
   "source": [
    "sanity_check(structured, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6def82a6-7e6f-47b9-b1bd-4769d7d4f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Turns: 6\n",
      "Max Turns: 79\n",
      "Avg Turns: 11\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(c[\"messages\"]) for c in structured]\n",
    "\n",
    "print(\"Min Turns:\", min(lengths))\n",
    "print(\"Max Turns:\", max(lengths))\n",
    "print(\"Avg Turns:\", sum(lengths)//len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5d0c733-ecac-4b79-9c84-cedd49174981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2092\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee55ebfc11e4c63b2eb1094eb14d30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2092 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "reddit_convo_ds = Dataset.from_list(structured)\n",
    "print(reddit_convo_ds)\n",
    "\n",
    "tokenized_dataset = reddit_convo_ds.map(\n",
    "    tokenize_fn,\n",
    "    remove_columns = [\"messages\"],\n",
    "    batched = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b45f4994-4656-4b14-bbff-2067cbe767ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2092\n",
      "})\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)\n",
    "print(tokenized_dataset[0].keys())\n",
    "print(len(tokenized_dataset[0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5379210-32ce-42d4-9b39-69d43c2ff95b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16b74dc5863443492e43deb7f4c65c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec9bd0f6e46446ca5f0b4a4ff7ba8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data.csv:   0%|          | 0.00/26.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8f0534bdeb4718ba729260368e2e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/397928 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comment': '$100,000 liberal arts degree, works at Starbucks. \\n\\nClaims to be an “old soul” yet has never gone more than an hour without WiFi.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "roast_ds = load_dataset(\n",
    "    \"gus-gustavo/reddit_roastme\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(roast_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "508200fc-af18-4cb3-bdb2-8630ac1093ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cabcedb9e443b59c46730e19c895c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/397928 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered roast samples: 376117\n",
      "{'comment': '$100,000 liberal arts degree, works at Starbucks. \\n\\nClaims to be an “old soul” yet has never gone more than an hour without WiFi.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_roast(text):\n",
    "    text = text.strip()\n",
    "    if len(text) < 15:\n",
    "        return False\n",
    "    if not re.search(r\"[a-zA-Z]\", text):\n",
    "        return False\n",
    "    if text.lower() in {\"you\", \"no\", \"there\", \"go\", \"it\", \"100%\"}:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "roast_ds = roast_ds.filter(lambda x: is_valid_roast(x[\"comment\"]))\n",
    "\n",
    "print(\"Filtered roast samples:\", len(roast_ds))\n",
    "print(roast_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d85a0d8-8ec3-410f-94da-ca06d0c64873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bdd2539fa94020bdb3e1b4aeddfaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/376117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INSULT_PROMPTS = [\n",
    "    \"you look dumb\",\n",
    "    \"lmao you’re ugly\",\n",
    "    \"you’re cringe\",\n",
    "    \"bro you’re embarrassing\",\n",
    "    \"why do you look like that\",\n",
    "    \"nah you got no personality\",\n",
    "]\n",
    "\n",
    "import random\n",
    "\n",
    "def roast_to_chat(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": random.choice(INSULT_PROMPTS),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example[\"comment\"],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "roast_chat = roast_ds.map(\n",
    "    roast_to_chat,\n",
    "    remove_columns=roast_ds.column_names\n",
    ")\n",
    "\n",
    "print(roast_chat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "074326c3-0e37-4b97-80b6-c29a6018b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roast Sample Kept: 56417\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "roast_chat = roast_chat.shuffle(seed=43).select(\n",
    "    range(int(len(roast_chat) * 0.15))\n",
    ")\n",
    "\n",
    "print(\"Roast Sample Kept:\", len(roast_chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eda9dc9-384c-4cc2-abd6-7968f2ec0dd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4031be068f1f4525b40ac6800027f776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/267 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3709880630b44326a5151e0574142f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/15.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b80fef3d8f49e49f4181b500ad1bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '• “Is your name Google? Because you’re everything I’ve been searching for.”'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rizz_ds = load_dataset(\"Shaheer-ipynb/Rizz-Dataset\", split=\"train\")\n",
    "print(rizz_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "424ab6d6-9134-4e2c-8eea-ec73a81eb435",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'Yes', 'role': 'user'}, {'content': 'Sure ig', 'role': 'assistant'}, {'content': 'Just add me', 'role': 'user'}, {'content': 'Done Briv', 'role': 'assistant'}, {'content': 'Yeah bro', 'role': 'user'}, {'content': 'Help-', 'role': 'assistant'}, {'content': 'Tell', 'role': 'user'}, {'content': 'Rate it? ☠', 'role': 'assistant'}, {'content': \"It's actually good\", 'role': 'user'}, {'content': '8.8/10', 'role': 'user'}, {'content': 'Bro u always look killer 💪💪', 'role': 'user'}, {'content': \"First edit in a life- ☠ nah bro I don't-\", 'role': 'assistant'}, {'content': 'Nah bro you are... ☠️', 'role': 'user'}, {'content': '☠️ (Reposted Nothing Changed-)', 'role': 'assistant'}, {'content': 'Bro u just need a sexy girl 🙂', 'role': 'user'}, {'content': 'Nah ☠ why-', 'role': 'assistant'}, {'content': \"Cuz' u r sexy \\nLHS=RHS\", 'role': 'user'}, {'content': 'Whats LHS and RHS Doing in my life They were already bad enough in maths 😭😭', 'role': 'assistant'}, {'content': 'I meant to say that u r sexy so u deserve the sexy one', 'role': 'user'}, {'content': 'Damn ☠ Bruv why this getting views-', 'role': 'assistant'}, {'content': \"u r fucking sexy that's y☠☠\", 'role': 'user'}, {'content': 'Nah still tho I hope it stops at 299 now- Thats enough people already saw me-', 'role': 'assistant'}, {'content': 'u r right☠', 'role': 'user'}, {'content': \"ikr It should stop now Oh well it didn't-\", 'role': 'assistant'}, {'content': '3 more☠', 'role': 'user'}, {'content': 'Damn Not 3', 'role': 'assistant'}, {'content': 'dam bruh!!!!!!!!', 'role': 'user'}, {'content': 'its 30💀', 'role': 'user'}, {'content': 'Its gonna stop now-', 'role': 'assistant'}, {'content': 'it should☠☠☠☠☠', 'role': 'user'}, {'content': 'i m getting jelousssssss', 'role': 'user'}, {'content': 'Well its stopping', 'role': 'assistant'}, {'content': 'dam', 'role': 'user'}, {'content': 'Oh yea it totally gonna stop now-', 'role': 'assistant'}, {'content': 'dam', 'role': 'user'}, {'content': 'hope it gonna stop', 'role': 'user'}, {'content': 'Bruh this literally me ☠', 'role': 'assistant'}, {'content': 'how old ago?', 'role': 'user'}, {'content': 'years-', 'role': 'assistant'}, {'content': 'wanna see mine?', 'role': 'user'}, {'content': 'Yeaa', 'role': 'assistant'}, {'content': 'i had look even worst', 'role': 'user'}, {'content': 'Damn Gimme the pics', 'role': 'assistant'}, {'content': 'i dont have phone now', 'role': 'user'}, {'content': 'i will send it at 11 pm', 'role': 'user'}, {'content': 'Aw hell naw Tf is this-', 'role': 'assistant'}, {'content': 'bro before🙂 and now☠', 'role': 'user'}, {'content': 'Bro this literally me-', 'role': 'assistant'}, {'content': 'bro u r fair', 'role': 'user'}, {'content': 'Sometimes-', 'role': 'assistant'}, {'content': 'i dont even look so fair', 'role': 'user'}, {'content': 'u r litteraly looking like a jharkhandian like me👽', 'role': 'user'}, {'content': 'From where do you think Im fair- Deadass eyes Ong', 'role': 'assistant'}, {'content': \"bro that's me👽\", 'role': 'user'}, {'content': 'i look exactly same', 'role': 'user'}, {'content': \"That me- Jharkhandi- Thats why I say I don't look good-\", 'role': 'assistant'}, {'content': 'bro your looks🙂 ur english ☠☠', 'role': 'user'}, {'content': 'i can understand', 'role': 'user'}, {'content': 'This me last january-', 'role': 'assistant'}, {'content': 'but the photos in your profile are with very danger looks💀☠', 'role': 'user'}, {'content': 'My english☠️', 'role': 'assistant'}, {'content': \"what's your height\", 'role': 'user'}, {'content': \"5'10\", 'role': 'assistant'}, {'content': \"me 5'8\", 'role': 'user'}, {'content': 'this photo☠☠', 'role': 'user'}, {'content': \"Damn I'm close to 5'11\", 'role': 'assistant'}, {'content': 'dam bro', 'role': 'user'}, {'content': \"Yea I was stuck at 5'6 last january-\", 'role': 'assistant'}, {'content': 'fuck !!!!!!1 bruh ☠', 'role': 'user'}, {'content': \"Thats why I say I don't look good-\", 'role': 'assistant'}, {'content': 'yeah now i am understanding☠👽', 'role': 'user'}, {'content': 'From this to This', 'role': 'assistant'}, {'content': 'bro u r exactly same as me', 'role': 'user'}, {'content': 'bruh!! this photo got me weak💀', 'role': 'user'}, {'content': 'what do you think how i look?', 'role': 'user'}, {'content': 'Like?? Wha- Where pics??', 'role': 'assistant'}, {'content': 'i dont have phone now☠', 'role': 'user'}, {'content': 'i will get the phone at 11☠', 'role': 'user'}, {'content': 'Damn I See', 'role': 'assistant'}, {'content': \"mom and dad kept me in controll and didn't get me a phone\", 'role': 'user'}, {'content': \"i use my mom's phone\", 'role': 'user'}, {'content': 'uss', 'role': 'assistant'}, {'content': \"and my brother's laptop for studies💀☠\", 'role': 'user'}, {'content': 'only laptop', 'role': 'assistant'}, {'content': 'its not even mine bruh', 'role': 'user'}, {'content': 'Damn', 'role': 'assistant'}, {'content': 'i am using this on incognito mode☠', 'role': 'user'}, {'content': 'Although Its for me and my lil bro', 'role': 'assistant'}, {'content': 'dam 🗿', 'role': 'user'}, {'content': 'Ah shi-', 'role': 'assistant'}, {'content': 'shit', 'role': 'user'}, {'content': 'its increasing', 'role': 'user'}, {'content': 'bro but your looks getting good day by day', 'role': 'user'}, {'content': \"Um It wasn't till last jan-\", 'role': 'assistant'}, {'content': 'Bro', 'role': 'user'}, {'content': 'Yeah? ☠️', 'role': 'assistant'}, {'content': 'Look at me bro ☠️', 'role': 'user'}, {'content': '2019', 'role': 'user'}, {'content': \"Damn this one's cute ngl You would've been my best friend\", 'role': 'assistant'}, {'content': 'nah bro its not☠', 'role': 'user'}, {'content': \"It is If you've met me irl\", 'role': 'assistant'}, {'content': 'dam bro  🗿🤝', 'role': 'user'}, {'content': 'Ikr', 'role': 'assistant'}, {'content': 'bro we are exactly same  🗿', 'role': 'user'}, {'content': \"Ikr Btw it's at 417 views now-\", 'role': 'assistant'}, {'content': 'dam bruh', 'role': 'user'}, {'content': 'people ( girls💀 ) liked your reels', 'role': 'user'}, {'content': 'Idk ☠️ But well some new follows', 'role': 'assistant'}, {'content': 'dam  🗿', 'role': 'user'}, {'content': 'u deserve', 'role': 'user'}, {'content': 'bro check this my id', 'role': 'user'}, {'content': 'hexed_harmonie_', 'role': 'user'}, {'content': 'there is my dp', 'role': 'user'}, {'content': 'I already checked it', 'role': 'assistant'}, {'content': 'bro ik i am not so cool💀', 'role': 'user'}, {'content': 'Bro you can be ☠️', 'role': 'assistant'}, {'content': 'nah bro☠', 'role': 'user'}, {'content': 'i am just looking normal', 'role': 'user'}, {'content': 'Us bro (you already saw my old pics🗿)', 'role': 'assistant'}, {'content': 'that photo is 9 moths ago photo', 'role': 'user'}, {'content': 'yeah', 'role': 'user'}, {'content': 'i will send you my recent one', 'role': 'user'}, {'content': 'Nah bro 5 or 6 months old- Send', 'role': 'assistant'}, {'content': 'not now but after 1 hour ☠', 'role': 'user'}, {'content': 'i dont have phone', 'role': 'user'}, {'content': 'according to you how do i am looking i that dp 💀', 'role': 'user'}, {'content': 'I see Well send me after an hour then- Which dp?', 'role': 'assistant'}, {'content': 'that another id', 'role': 'user'}, {'content': 'Girl But the tanjiro name is there-', 'role': 'assistant'}, {'content': 'hexed_harmonie_', 'role': 'user'}, {'content': 'no bro', 'role': 'user'}, {'content': 'this is my another id', 'role': 'user'}, {'content': 'check', 'role': 'user'}, {'content': 'You look good Ngl', 'role': 'assistant'}, {'content': 'i am looking like fat guy☠', 'role': 'user'}, {'content': 'Kind of But that can be improved I believe in potential', 'role': 'assistant'}, {'content': 'because that phot is 9 moth old', 'role': 'user'}, {'content': 'i am gonna send the recent one', 'role': 'user'}, {'content': 'may be i have improved myself little bit', 'role': 'user'}, {'content': 'Yep Send', 'role': 'assistant'}, {'content': 'after 1 hour ☠', 'role': 'user'}, {'content': 'Sure I will be waiting ☠️', 'role': 'assistant'}, {'content': 'yup', 'role': 'user'}, {'content': 'Recent one 💀', 'role': 'user'}, {'content': 'Damn you look cute', 'role': 'assistant'}, {'content': 'Thanks bro 💀', 'role': 'user'}, {'content': 'No mentions But fr you got too much potential-', 'role': 'assistant'}, {'content': 'Samjh ni aya bhai?', 'role': 'user'}, {'content': 'What kind of potential?', 'role': 'user'}, {'content': 'Bro You got time?', 'role': 'assistant'}, {'content': 'Bhai kuch samjh ni arha', 'role': 'user'}, {'content': 'Hindi me bolde vro 😭', 'role': 'user'}, {'content': 'Cool vibe', 'role': 'user'}, {'content': 'Damn 💀 🤙', 'role': 'user'}, {'content': 'Wheres that? Rate', 'role': 'assistant'}, {'content': 'Damn 9.4/10', 'role': 'user'}, {'content': 'Wha-', 'role': 'assistant'}, {'content': 'bro u always come with damn things 🗿', 'role': 'user'}, {'content': '8.6/10', 'role': 'user'}, {'content': \"It wasn't that good tho\", 'role': 'assistant'}, {'content': 'bro i cant even able to draw the outlines and u draw it realasticaly🗿', 'role': 'user'}, {'content': 'Damn It is kinda wierd?', 'role': 'assistant'}, {'content': 'nothing wierd bro', 'role': 'user'}, {'content': 'its damn cool', 'role': 'user'}, {'content': 'It was this-', 'role': 'assistant'}, {'content': 'u transformed it from shit to damn 🗿 like yourself 🗿', 'role': 'user'}, {'content': 'Damn', 'role': 'assistant'}, {'content': 'From lomdiabazi 💀', 'role': 'user'}, {'content': 'damn  7.8/10 🗿', 'role': 'user'}, {'content': 'Nah', 'role': 'assistant'}, {'content': 'it is bro 🗿', 'role': 'user'}, {'content': '7.8', 'role': 'user'}, {'content': 'Nah 0.0', 'role': 'assistant'}, {'content': 'bro u deserve it bruh', 'role': 'user'}, {'content': 'Nuh uh', 'role': 'assistant'}, {'content': 'ok then i deserve 7.8 🗿', 'role': 'user'}, {'content': 'True True I Agree', 'role': 'assistant'}, {'content': 'bro🤝🗿', 'role': 'user'}, {'content': 'Damn Someone fainted', 'role': 'assistant'}, {'content': 'who ? me? 💀', 'role': 'user'}, {'content': 'Nah', 'role': 'assistant'}, {'content': 'damn fu- 💀', 'role': 'user'}, {'content': 'she?', 'role': 'user'}, {'content': 'Yea', 'role': 'assistant'}, {'content': 'gimme her id 🗿', 'role': 'user'}, {'content': 'Idk how But somehow Jien became my bestie-', 'role': 'assistant'}, {'content': 'did u post it ?', 'role': 'user'}, {'content': 'Nah Im scared ig-', 'role': 'assistant'}, {'content': 'oh she is jien????', 'role': 'user'}, {'content': 'jien?', 'role': 'user'}, {'content': 'Nah Nah', 'role': 'assistant'}, {'content': \"then who's she gimme her id🗿\", 'role': 'user'}, {'content': 'Well', 'role': 'assistant'}, {'content': 'True', 'role': 'user'}, {'content': 'What?', 'role': 'assistant'}, {'content': 'nthng', 'role': 'user'}, {'content': 'WydM Wyd?', 'role': 'assistant'}, {'content': 'are bhai nothing', 'role': 'user'}, {'content': 'To kuch karne ka socha hai?', 'role': 'assistant'}, {'content': 'kya? 💀', 'role': 'user'}, {'content': 'Kuch bhi?', 'role': 'assistant'}, {'content': 'bhai sochra hu ek bandi banalu 🗿', 'role': 'user'}, {'content': 'Am I cooked Patale', 'role': 'assistant'}, {'content': 'ye ladki ki id de 🗿', 'role': 'user'}, {'content': \"Look at her pfp She's following me\", 'role': 'assistant'}, {'content': 'bhot din hogye kisi ladki se pyar ki batein ni kiya 🗿', 'role': 'user'}, {'content': \"Damn Why don't you try ai-\", 'role': 'assistant'}, {'content': 'try what?', 'role': 'user'}, {'content': 'janitorai.com', 'role': 'assistant'}, {'content': 'artificial intelligence?', 'role': 'user'}, {'content': 'There are good girls to talk to Ai Girls-', 'role': 'assistant'}, {'content': 'bhai itna pagal thori na hu ki virtual girls se satified ho jau ☠', 'role': 'user'}, {'content': 'bhai tu konsa phone use karta ha 🗿', 'role': 'user'}, {'content': 'Vivo Wby?', 'role': 'assistant'}, {'content': 'itne baap photos ate ha konsa filter ha', 'role': 'user'}, {'content': 'Normal Filter Insta one', 'role': 'assistant'}, {'content': \"redmi 3 years old 10k price and it is my mum's 💀\", 'role': 'user'}, {'content': 'Damn Vivo Z1X Pro 5 Years Old Mums phoen Flex Hai', 'role': 'assistant'}, {'content': 'damn bruh 🗿', 'role': 'user'}, {'content': 'Ikr We flexin', 'role': 'assistant'}, {'content': 'bhai bas tere jaisa phone lena ha fir set ha sab 🗿', 'role': 'user'}, {'content': 'Mera phone 12k ka hai-', 'role': 'assistant'}, {'content': 'us bruh 🗿🗿🗿🗿🗿🗿', 'role': 'user'}, {'content': 'What happened bruh?', 'role': 'assistant'}, {'content': 'bro just kick her', 'role': 'user'}, {'content': 'I need a good reason for hta that', 'role': 'assistant'}, {'content': 'bhai chat padhle', 'role': 'user'}, {'content': 'uper', 'role': 'user'}, {'content': 'Bhai batate idhar', 'role': 'assistant'}, {'content': 'meri marzi mera mann meri gand me mera baap ka lund', 'role': 'user'}, {'content': 'ye kaha ha usne', 'role': 'user'}, {'content': 'for no reason', 'role': 'user'}, {'content': 'usko maine zyada izzat de di thi kal', 'role': 'user'}, {'content': 'bhai tu allow karta ha ki mai use gaali du?', 'role': 'user'}, {'content': 'bhai usne sorry tak ni kaha yar uper se chad ke baat karri', 'role': 'user'}, {'content': 'I See well I said her to apologize Lemme see If it can be resoved resolved', 'role': 'assistant'}, {'content': 'bhai itna izzat mat de bro', 'role': 'user'}, {'content': 'ek kaam karta hu', 'role': 'user'}, {'content': 'gc me usi ke language me gaali dedeta hu', 'role': 'user'}, {'content': 'apologize ni chaiye usse', 'role': 'user'}, {'content': 'Yea do that Do that Bruh I hate That sweetness manipulation Feeling cringe', 'role': 'assistant'}, {'content': 'means?', 'role': 'user'}, {'content': 'okie', 'role': 'user'}, {'content': 'i gottu', 'role': 'user'}, {'content': 'I mean I hate kam ke liye pyar dikhana Or something like that', 'role': 'assistant'}, {'content': 'smjh gya', 'role': 'user'}, {'content': 'bhai yar ni dunga gaali', 'role': 'user'}, {'content': 'chor', 'role': 'user'}, {'content': 'Why?', 'role': 'assistant'}, {'content': 'i am a kind person 🗿', 'role': 'user'}, {'content': 'Damn', 'role': 'assistant'}, {'content': 'sahi time ane de bro', 'role': 'user'}, {'content': 'ladki bolker chali gyi uper se bhav khaari', 'role': 'user'}, {'content': 'I See Damn', 'role': 'assistant'}, {'content': 'bhai kick marde', 'role': 'user'}, {'content': 'best option', 'role': 'user'}, {'content': 'sali ka ghamand tut jayega 1 sec me', 'role': 'user'}, {'content': \"Thats why I don't care about these thigns\", 'role': 'assistant'}, {'content': 'main devna se baat karlunga', 'role': 'user'}, {'content': 'Damn Changed', 'role': 'assistant'}, {'content': 'bhai imagine koi tereko itna ganda boldiya for no reason fir tereper he chadh kar bolrha', 'role': 'user'}, {'content': 'ghamand khara', 'role': 'user'}, {'content': 'uper se koi ladki', 'role': 'user'}, {'content': \"Well if she said that to me Then I wouldn't have kicked her Instead Prolly said *Damn but aren't you the one who's asshole is always filled with someone else's cock everyday* Nv,m\", 'role': 'assistant'}, {'content': 'bhai sidha baat kick karde sab khatam , ager devna boldegi kuch toh mai baat karlunga usse', 'role': 'user'}, {'content': 'she said to you??', 'role': 'user'}, {'content': 'Nah If she said that to me', 'role': 'assistant'}, {'content': 'u r imagining?', 'role': 'user'}, {'content': \"I wouldn't kick instead roasted her\", 'role': 'assistant'}, {'content': 'lagta ha maine kuch zyada he izzat dediya use', 'role': 'user'}, {'content': 'aur roast ni kiya', 'role': 'user'}, {'content': 'Damn I never do this', 'role': 'assistant'}, {'content': 'kuch zyada accha ban gya hu', 'role': 'user'}, {'content': 'Agar wo apne bounderies me ni hai to why me', 'role': 'assistant'}, {'content': 'bhai sunn', 'role': 'user'}, {'content': 'ager wo age se aisa kuch boli meko toh dekh lena tandav macha dunga', 'role': 'user'}, {'content': 'Abhi bhi macha de No problem', 'role': 'assistant'}, {'content': 'bhai trend khatam hogya', 'role': 'user'}, {'content': 'wo offline bhi ho gyi', 'role': 'user'}, {'content': 'chor', 'role': 'user'}, {'content': 'i am a kind person afterall', 'role': 'user'}, {'content': 'what kind of competition?', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "hf_dataset = Dataset.from_list(conversations)\n",
    "insta_ds = hf_dataset\n",
    "print(insta_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c9d79b4-7ef1-490f-aaa8-0d31504e9373",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_convs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m insta_new_ds = Dataset.from_list(\u001b[43mnew_convs\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(insta_new_ds[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'new_convs' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "insta_new_ds = Dataset.from_list(new_convs)\n",
    "print(insta_new_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91f5eb82-6bf8-4f82-b126-9d289be947d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Size: 17951\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "def rizz_to_chat(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"text\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "rizz_chat = rizz_ds.map(\n",
    "    rizz_to_chat,\n",
    "    remove_columns=rizz_ds.column_names\n",
    ")\n",
    "\n",
    "rizz_chat = rizz_chat.shuffle(seed=43).select(\n",
    "    range(int(len(rizz_chat) * 0.3))\n",
    ")\n",
    "\n",
    "merged_ds = concatenate_datasets([\n",
    "    insta_ds, \n",
    "    rizz_chat,\n",
    "    roast_chat,\n",
    "]).shuffle(seed=43)\n",
    "\n",
    "merged_ds = merged_ds.shuffle(seed=42).select(\n",
    "    range(int(len(merged_ds) * 0.3 ))\n",
    ")\n",
    "\n",
    "print(\"Final Dataset Size:\", len(merged_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f1f53d5-1e92-449b-9f86-89c4695efdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7d22f4bdd64c7db162112e992ab20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = insta_ds.map(\n",
    "    tokenize_fn,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b02adf2-af7a-47b4-924b-60fae7d20d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61252463e1f8442f954bc92914a12789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = insta_new_ds.map(\n",
    "    tokenize_fn,\n",
    "    remove_columns = [\"messages\"],\n",
    "    batched = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61a5367d-537d-4ddc-a51a-17e6aae1b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "input_ids len: 1024\n",
      "labels match: True\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0].keys())\n",
    "print(\"input_ids len:\", len(tokenized_dataset[0][\"input_ids\"]))\n",
    "print(\"labels match:\", tokenized_dataset[0][\"input_ids\"] == tokenized_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53310387-2a9e-4937-abd6-41e40541e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05834080-f8fd-4d7c-8e0a-c780528c4117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d51dd9f7-7f0e-4c9d-ba07-20315fcf3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DefaultPC\\WebApplications\\AiPractices\\Elevia\\ai-venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\DefaultPC\\WebApplications\\AiPractices\\Elevia\\ai-venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-llama3-rizz/Training3\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a82f44e7-e4db-4132-bc57-f69011bf4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a11fa5b1-2a29-4f65-a505-aa4fa5c744c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='814' max='814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [814/814 2:00:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.434600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.352700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.345100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=814, training_loss=0.37736206324153215, metrics={'train_runtime': 7249.323, 'train_samples_per_second': 0.449, 'train_steps_per_second': 0.112, 'total_flos': 1.5040769345716224e+17, 'train_loss': 0.37736206324153215, 'epoch': 0.9993861264579497})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "297ab895-8c7f-4f03-8a94-067f2a555978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6841c020bf4c4e9b07fce078bea50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./lora-llama3-rizz/roast/checkpoint-815\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Trained Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b04f5d4-9e22-4b65-ac2e-105c60be1147",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m.apply_chat_template(\n\u001b[32m      4\u001b[39m     messages,\n\u001b[32m      5\u001b[39m     tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m ).to(model.device)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     11\u001b[39m     output = model.generate(\n\u001b[32m     12\u001b[39m         inputs,\n\u001b[32m     13\u001b[39m         max_new_tokens=\u001b[32m200\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         repetition_penalty=\u001b[32m1.0\u001b[39m\n\u001b[32m     18\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=1.5,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.0\n",
    "    )\n",
    "\n",
    "generated_ids = output[0][inputs.shape[-1]:]\n",
    "\n",
    "import re\n",
    "\n",
    "text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "# Remove role tokens even if glued to text\n",
    "text = re.sub(r\"(system|user|assistant)\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "# Remove metadata lines\n",
    "clean_lines = []\n",
    "for line in text.split(\"\\n\"):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    if line.startswith(\"Cutting Knowledge Date:\"):\n",
    "        continue\n",
    "    if line.startswith(\"Today Date:\"):\n",
    "        continue\n",
    "    clean_lines.append(line)\n",
    "\n",
    "text = \"\\n\".join(clean_lines)\n",
    "bubbles = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "if len(bubbles) == 1:\n",
    "    if random.random() < 0.9:\n",
    "        # sentence-based split\n",
    "        bubbles = re.split(r\"[.!?]\\s+\", bubbles[0])\n",
    "\n",
    "\n",
    "ASK_BACK_VARIANTS = [\"wbu?\", \"what about you?\", \"hbu?\", \"you?\"]\n",
    "EMOJIS = [\"😭😭\", \"💔🥀\", \"🥀\", \"😭🙏\"]\n",
    "\n",
    "new_bubbles = []\n",
    "\n",
    "for i, bubble in enumerate(bubbles):\n",
    "    modified = bubble\n",
    "\n",
    "    # Emoji policy (per bubble)\n",
    "    if not any(e in modified for e in EMOJIS):\n",
    "            modified += \" \" + random.choice(EMOJIS)\n",
    "\n",
    "    # Ask-back policy (ONLY on last bubble)\n",
    "    if i == len(bubbles) - 1:\n",
    "        if \"?\" not in modified and random.random() < 0.85:\n",
    "            modified += \" \" + random.choice(ASK_BACK_VARIANTS)\n",
    "\n",
    "    new_bubbles.append(modified)\n",
    "\n",
    "bubbles = new_bubbles\n",
    "\n",
    "print(bubbles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ec0cf5b-b4b0-4cee-a0ca-8e411eb82840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL REPLY (RAW, NO LOGIC) ===\n",
      "Helloo What 😿🥀 🫂👋🙏\n"
     ]
    }
   ],
   "source": [
    "# === CHAT WITH MEMORY + NEW USER MESSAGE ===\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# -------- CONFIG --------\n",
    "CHAT_JSON_PATH = \"chats.json\"\n",
    "MAX_TURNS = 30\n",
    "MAX_NEW_TOKENS = 400\n",
    "\n",
    "# -------- NEW USER MESSAGE --------\n",
    "new_user_message = \"HELLOOO\"\n",
    "\n",
    "# -------- LOAD CHAT --------\n",
    "with open(CHAT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    chat_data = json.load(f)\n",
    "\n",
    "# -------- NORMALIZE CHAT FORMAT --------\n",
    "if isinstance(chat_data, list) and isinstance(chat_data[0], dict) and \"role\" in chat_data[0]:\n",
    "    messages = chat_data\n",
    "\n",
    "elif isinstance(chat_data, dict) and \"messages\" in chat_data:\n",
    "    messages = chat_data[\"messages\"]\n",
    "\n",
    "elif isinstance(chat_data, list) and \"messages\" in chat_data[0]:\n",
    "    messages = chat_data[0][\"messages\"]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unknown chats.json structure\")\n",
    "\n",
    "# -------- BUILD CONTEXT WINDOW --------\n",
    "context_messages = messages[-(MAX_TURNS - 1):]\n",
    "\n",
    "# ADD NEW USER MESSAGE\n",
    "context_messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": new_user_message\n",
    "})\n",
    "\n",
    "# -------- SYSTEM PROMPT --------\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"You are Elen. \"\n",
    "        \"You text casually like a real person on Instagram. \"\n",
    "        \"You reply naturally based on the conversation.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "prompt_messages = [system_prompt] + context_messages\n",
    "\n",
    "# -------- TOKENIZE --------\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# -------- GENERATE --------\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.9,\n",
    "        top_p=0.85,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "# -------- SLICE ONLY NEW TOKENS --------\n",
    "generated_ids = output[0][inputs.shape[-1]:]\n",
    "\n",
    "reply = tokenizer.decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "print(\"=== MODEL REPLY (RAW, NO LOGIC) ===\")\n",
    "print(reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI VENV (3.11)",
   "language": "python",
   "name": "ai-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
